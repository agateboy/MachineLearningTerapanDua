# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1atNjyyRoyCsXm3Zdyo7cavwR7J28DmCw

# Data Collection
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_squared_error
from math import sqrt
import matplotlib.pyplot as plt
import seaborn as sns

!pip install kaggle
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d CooperUnion/anime-recommendations-database

!unzip anime-recommendations-database.zip

anime = pd.read_csv('anime.csv')
ratings = pd.read_csv('rating.csv')

"""# Data Understanding

## Data Anime
"""

anime.head(10)

anime.shape

anime.info()

anime.describe()

"""## Data Rating"""

ratings.head(10)

ratings.shape

ratings.info()

ratings.describe()

"""#Exploratory Data Analysis"""

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 8))
anime['type'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Anime Types')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(anime['rating'].dropna(), bins=20, kde=True)
plt.title('Distribution of Anime Average Ratings')
plt.xlabel('Average Rating')
plt.ylabel('Number of Anime')
plt.show()

top_community_anime = anime.sort_values(by='members', ascending=False).head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x='members', y='name', data=top_community_anime, palette='viridis')
plt.title('Top 10 Anime by Community Size')
plt.xlabel('Number of Members')
plt.ylabel('Anime Name')
plt.show()

top_rated_anime = anime.sort_values(by='rating', ascending=False).head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x='rating', y='name', data=top_rated_anime, palette='magma')
plt.title('Top 10 Anime by Average Rating')
plt.xlabel('Average Rating')
plt.ylabel('Anime Name')
plt.xlim(top_rated_anime['rating'].min() * 0.95, top_rated_anime['rating'].max() * 1.05) # Adjust x-axis limits for better visualization
plt.show()

"""# Data Preparation"""

anime.isnull().sum()

anime.dropna(subset=['name'], inplace=True)

anime['genre'] = anime['genre'].fillna('')

anime.isna().sum()

ratings.isna().sum()

ratings = ratings[ratings['rating'] != -1]

ratings.head(10)

"""## TF-IDF Vectorization"""

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(anime['genre'])

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

indices = pd.Series(anime.index, index=anime['name']).drop_duplicates()

"""# Model Development Content Based Filtering"""

def get_content_recommendations(title, top_n=10):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    anime_indices = [i[0] for i in sim_scores]
    return anime[['name', 'genre']].iloc[anime_indices]

print("\nContent-Based Top 10 Recommendations for 'Naruto':")
print(get_content_recommendations("Naruto"))

import numpy as np
from sklearn.model_selection import train_test_split

train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)

def evaluate_content_based(model_recommendation_function, train_data, test_data, k=10):
    actual_items = test_data.groupby('user_id')['anime_id'].apply(list).to_dict()

    precision_list = []
    recall_list = []

    test_users = test_data['user_id'].unique()

    for user_id in test_users:
        user_anime_train = train_data[train_data['user_id'] == user_id]['anime_id'].tolist()

        if not user_anime_train:
            continue

        sample_anime_id = user_anime_train[0]
        sample_anime_name = anime[anime['anime_id'] == sample_anime_id]['name'].iloc[0]

        recommended_anime_names = model_recommendation_function(sample_anime_name, top_n=k)['name'].tolist()

        recommended_anime_ids = anime[anime['name'].isin(recommended_anime_names)]['anime_id'].tolist()

        actual_rated_anime_ids = actual_items.get(user_id, [])

        relevant_recommended_items = len(set(recommended_anime_ids) & set(actual_rated_anime_ids))

        if len(recommended_anime_ids) > 0:
            precision = relevant_recommended_items / len(recommended_anime_ids)
            precision_list.append(precision)

        if len(actual_rated_anime_ids) > 0:
            recall = relevant_recommended_items / len(actual_rated_anime_ids)
            recall_list.append(recall)

    avg_precision = np.mean(precision_list) if precision_list else 0
    avg_recall = np.mean(recall_list) if recall_list else 0

    return avg_precision, avg_recall

avg_precision_cb, avg_recall_cb = evaluate_content_based(get_content_recommendations, train_ratings, test_ratings, k=10)

print(f"\nAverage Precision for Content-Based Filtering: {avg_precision_cb:.4f}")
print(f"Average Recall for Content-Based Filtering: {avg_recall_cb:.4f}")

"""# Model Development Collaborative Filtering (User-Based)"""

user_anime_matrix = ratings.pivot_table(index='user_id', columns='anime_id', values='rating')

user_anime_matrix_filled = user_anime_matrix.fillna(0)

user_similarity = cosine_similarity(user_anime_matrix_filled)
user_sim_df = pd.DataFrame(user_similarity, index=user_anime_matrix.index, columns=user_anime_matrix.index)

def predict_rating(user_id, anime_id):
    if anime_id not in user_anime_matrix.columns:
        return np.nan
    sim_scores = user_sim_df[user_id]
    ratings_for_anime = user_anime_matrix[anime_id]
    valid = ratings_for_anime[ratings_for_anime.notna()]
    sim_scores = sim_scores[valid.index]
    if sim_scores.sum() == 0:
        return np.nan
    return np.dot(sim_scores, valid) / sim_scores.sum()

def get_top_n_recommendations(user_id, n=10):
    watched = ratings[ratings['user_id'] == user_id]['anime_id']
    anime_ids = user_anime_matrix.columns.difference(watched)
    predictions = [(anime_id, predict_rating(user_id, anime_id)) for anime_id in anime_ids]
    predictions = [p for p in predictions if not np.isnan(p[1])]
    top_n = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]
    anime_names = anime.set_index('anime_id').loc[[i[0] for i in top_n]]['name']
    return anime_names.reset_index(drop=True)

print("\nCollaborative Top 10 Recommendations for User 1:")
print(get_top_n_recommendations(1))

"""# Evaluation Metric"""

sample_ratings = ratings.sample(1000, random_state=1)
preds = []
actuals = []

for _, row in sample_ratings.iterrows():
    pred = predict_rating(row['user_id'], row['anime_id'])
    if not np.isnan(pred):
        preds.append(pred)
        actuals.append(row['rating'])

rmse = sqrt(mean_squared_error(actuals, preds))
print(f"\nRMSE for Collaborative Filtering on sample: {rmse:.4f}")